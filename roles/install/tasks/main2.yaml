- name: Install and Configure Apache Spark and Hive Metastore
  hosts: all
  become: yes
  vars_files:
    - vars/main.yml
  

  tasks:

    - name: Install acl package
      apt:
        name: acl
        state: present

    - name: Install build-essential package
      apt:
        name: build-essential
        state: present

    - name: Install PostgreSQL server development libraries
      apt:
        name: postgresql-server-dev-all
        state: present

    - name: Install psycopg2 dependencies
      pip:
        name: psycopg2

    - name: Create Hive Metastore database and user
      become_user: postgres
      postgresql_db:
        name: hivemetastore
        state: present

    - name: Create Hive Metastore user
      become_user: postgres
      postgresql_user:
        name: hiveuser
        password: hivepassword
        state: present

    - name: Download Apache Hadoop tarball
      get_url:
        url: "https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz"
        dest: "/tmp/hadoop-3.3.4.tar.gz"

    - name: Extract Apache Hadoop tarball
      unarchive:
        src: "/tmp/hadoop-3.3.4.tar.gz"
        dest: "/usr/local/"
        remote_src: yes
        creates: "/usr/local/hadoop"  # Prevent extraction if already extracted

    - name: Rename Hadoop directory
      command: mv /usr/local/hadoop-3.3.4 /usr/local/hadoop

    - name: Set HADOOP_HOME environment variable in /etc/profile
      lineinfile:
        path: "/etc/profile"
        line: "export HADOOP_HOME=/usr/local/hadoop"
        state: present

    - name: Add Hadoop to PATH in /etc/profile
      lineinfile:
        path: "/etc/profile"
        line: "export PATH=$PATH:$HADOOP_HOME/bin"
        state: present

    - name: Reload environment variables
      shell: source /etc/profile
      args:
        executable: /bin/bash

    - name: Upload Jinja2 template for core-site.xml
      copy:
        src: "/Users/adixit/trino-build/roles/install/templates/core-site.xml.j2"
        dest: "/usr/local/hadoop/etc/hadoop/core-site.xml"
        owner: root
        group: root
        mode: '0644'

    - name: Update hadoop-env.sh for Hadoop AWS tools
      lineinfile:
        path: "/usr/local/hadoop/etc/hadoop/hadoop-env.sh"
        line: "export HADOOP_OPTIONAL_TOOLS=\"hadoop-aws\""
        state: present

    - name: Verify Hadoop access to FlashBlade Object Store
      command: hadoop fs -ls s3a://<bucket name>
      register: hadoop_test
      ignore_errors: yes

    - name: Download Apache Hive tarball
      get_url:
        url: "https://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz"
        dest: "/tmp/apache-hive-3.1.3-bin.tar.gz"

    - name: Extract Apache Hive tarball
      unarchive:
        src: "/tmp/apache-hive-3.1.3-bin.tar.gz"
        dest: "/usr/local/"
        remote_src: yes
        creates: "/usr/local/hive"  # Prevent extraction if already extracted

    - name: Rename Hive directory
      command: mv /usr/local/apache-hive-3.1.3-bin /usr/local/hive

    - name: Set HIVE_HOME environment variable in /etc/profile
      lineinfile:
        path: "/etc/profile"
        line: "export HIVE_HOME=/usr/local/hive"
        state: present

    - name: Add Hive to PATH in /etc/profile
      lineinfile:
        path: "/etc/profile"
        line: "export PATH=$PATH:$HIVE_HOME/bin"
        state: present

    - name: Reload environment variables
      shell: source /etc/profile
      args:
        executable: /bin/bash

    - name: Install required packages
      apt:
        name:
          - build-essential
          - postgresql-server-dev-all
          - acl
        state: present

    - name: Download Postgres Connector JAR
      get_url:
        url: https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.0/postgresql-42.7.0.jar
        dest: /tmp/postgresql-42.7.0.jar

    - name: Copy Postgres Connector JAR to Hive lib directory
      copy:
        src: /tmp/postgresql-42.7.0.jar
        dest: "{{ hive_home }}/lib/postgresql-42.7.0.jar"
        remote_src: yes

    - name: Copy Postgres Connector JAR to Spark jars directory
      copy:
        src: /tmp/postgresql-42.7.0.jar
        dest: "{{ spark_home }}/jars/postgresql-42.7.0.jar"
        remote_src: yes

    - name: Ensure Hive configuration directory exists
      file:
        path: "{{ hive_home }}/conf"
        state: directory

    - name: Copy hive-default.xml.template to hive-site.xml
      command: cp hive-default.xml.template hive-site.xml
      args:
        chdir: "{{ hive_home }}/conf"

    - name: Upload Jinja2 template for hive-site.xml
      template:
        src: /Users/adixit/trino-build/roles/install/templates/hive-site.xml.j2
        dest: "{{ hive_home }}/conf/hive-site.xml"

    - name: Initialize Hive Metastore Schema
      command: "{{ hive_home }}/bin/schematool -dbType postgres -initSchema"
      args:
        chdir: "{{ hive_home }}/bin"
      register: init_metastore
      failed_when: "'ERROR' in init_metastore.stderr"

    - name: Grant permissions on schema public for Hive user
      become_user: postgres
      postgresql_db:
        name: hivemetastore
        state: present

    - name: Grant CREATE permission to Hive user
      postgresql_query:
        query: "GRANT CREATE ON SCHEMA public TO hiveuser;"
        db: hivemetastore

  handlers:
    - name: Extract Spark
      unarchive:
        src: "/tmp/spark-3.5.2-bin-hadoop3.tgz"
        dest: "/usr/local/"
        remote_src: yes

    - name: Restart Hive
      service:
        name: hive-server2
        state: restarted



        
